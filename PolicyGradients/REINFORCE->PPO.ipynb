{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From REINFORCE to PPO:\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train REINFORCE with OpenAI Gym's Cartpole environment and cover the concepts behind REINFORCE. Then we will discuss its issues and finally learn how we can adress them using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# Set the device on which a torch.Tensor will be allocated.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define your NN architecture\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        # Set the corresponding action space size in the output layer\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Re-scale the raw scores (aka logits) so that the elements lie in range [0..1] \n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Convert state from numpy array to torch tensor and assign to it the device where it will be allocated\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        # Given a state, return the action probabilities from and .cpu() to copy its values in CPU memory\n",
    "        probs = self.forward(state).cpu()\n",
    "        # Categorical creates a categoricaldistribution parametrized by either probs or logits\n",
    "        # A categorical distribution is a discrete probability distribution that describes the possible results \n",
    "        # of a random variable that can take on one of K possible categories, with the probability of each category\n",
    "        # separately specified\n",
    "        # So here the random variable can take one of the ActionSpaceSize possible actions, and the given probabilities \n",
    "        # are the output of the NN\n",
    "        m = Categorical(probs)\n",
    "        # Given the probability distribution m the sample method will sample an action\n",
    "        action = m.sample()\n",
    "        # Log_prob method will calculate the log_probabilty of the sample action under the distribution m\n",
    "        # item() returns the value of a tensor as a standard python number\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect the allocation of NN tensors \n",
    "policy = Policy().to(device)\n",
    "# Set the optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Set a maximum timelimit for episode\n",
    "        for t in range(max_t):\n",
    "            # Given the state and under the policy theta return the sample action and its log prob\n",
    "            action, log_prob = policy.act(state)\n",
    "            # Need to collect all log probs for further calculation\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # as well as rewards\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        # According to the range of rewards prepare gammas' values\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        # Total discounted reward R(𝜏) = Σi=0 γi ri\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            # Loss equal to the negated log-probability of the action taken multiplied by the total discounted\n",
    "            # reward of tau Loss \n",
    "            policy_loss.append(-log_prob * R)\n",
    "        # Sum the losses \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the gradient\n",
    "        policy_loss.backward()\n",
    "        # Update the gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Gradient is Noisy, gradients have High variance...\n",
    "* We take one trajectory, calculate the Noise, compute the gradient, update the policy and so on ...\n",
    "Recall that$$\\nabla_{\\theta}U(\\theta)= E_{\\tau \\sim \\pi_{\\theta}(\\tau) } \\nabla_{\\theta} \\log \\pi_{\\theta}(\\tau) \n",
    "r(\\tau)\n",
    "$$\n",
    "Here, tau is the trajectory (the sequences of states st and actions at) the agent takes through a given sample, say from timestep 1 to T\n",
    "$$  \\log \\pi_{\\theta}(\\tau) = \\sum_{t=1}^{t=T} \\log \\pi_{\\theta}(a_{t} | s_{t})$$  and $$r(\\tau) = \\sum_{t=1}^{t=T} r(s_{t},a_{t})$$\n",
    "The typical way we compute this gradient is to sample a bunch of trajectories from the current policy 𝜏 ∼ 𝜋𝜃(𝜏) and average their values (the value we’re taking the expectation over). Then the computed gradient becomes dependent on the randomly sampled trajectories: $$\\nabla_{\\theta}U(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} \\log \\pi_{\\theta}(\\tau)  r(\\tau)$$\n",
    "As a result, it’s going to have variance, since its values depend on the sampled trajectories.     \n",
    "\n",
    "\n",
    "![Graph from CS294](img/SampledTrajectories.png)  \n",
    "\n",
    "This variance can be quite high. This is because each trajectory can take very different paths depending on the states visited and actions sampled from your policy, as shown in the graph above. This means that ∇𝜃 log 𝜋𝜃(𝜏) and  𝑟(𝜏) take on wildly different values depending on the trajectory.      \n",
    "\n",
    "Of course, the first easiest option is that if we scale N→∞, the average values will approach your true expected value, but we can’t afford to make N too high due to computational cost.\n",
    "\n",
    "### 2. Credit Assignment:\n",
    "* If the current policy gradient can be written as:\n",
    "\n",
    "$$\\nabla_{\\theta}U(\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N}   \\sum_{t=1}^{t=T} \\log \\pi_{\\theta}(a_{t} | s_{t})\\sum_{t=1}^{t=T} r(s_{t},a_{t})$$\n",
    "\n",
    "Then we can instead write it as:\n",
    "\n",
    "$$\\nabla_{\\theta}U(\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N}   \\sum_{t=1}^{t=T} \\log \\pi_{\\theta}(a_{t} | s_{t})\\sum_{t=t^{'}}^{t=T} r(s_{t^{'}},a_{t^{'}})$$\n",
    "We switched from t to t. As we are in Markov process, this basically says that the action at time-step t can only affect the future reward, so the past reward shouldn’t be contributing to the policy gradient. So a better policy gradient would simply have the future reward as the coefficient.  \n",
    "This allows:\n",
    "* To properly assign credits to the actions at.\n",
    "* Also reduces the number of random variables and consequently reduces the variance of the gradients.\n",
    "\n",
    "### 3. The update process is inefficient:\n",
    "1. Run the policy once.\n",
    "2. Update once.\n",
    "3. Throw away the trajectory.  \n",
    "\n",
    "What about recycling the trajectories and modifying them so that they become representative of the new policy and then use the recycled trajectories to compute the gradients and update our policy again and again.  \n",
    "\n",
    "To mitigate this inefficiency issue, we should use Importing Sampling.\n",
    "It allows us to estimate properties of a particular distribution while only having samples generated from a different distribution than the distribution of interest.  \n",
    "\n",
    "In our case, the generated trajectories with the policy 𝜋𝜃 have a probability P(𝜏,𝜃) to be sampled. If by chance the same trajectory can be sampled under another updated policy with a different probability P(𝜏,𝜃'). If we want to compute the average of some quantity, say f(𝜏), mathematically it's equivalent to:  \n",
    "\n",
    "$$ \\sum_{\\tau} P(\\tau;\\theta^{'}) f(\\tau) $$\n",
    "We could modify this equation by multiplying and dividing by the same number which is 𝑃(𝜏;𝜃′) and rearrange the terms:  \n",
    "$$ \\sum_{\\tau} P(\\tau;\\theta) \\frac{P(\\tau;\\theta^{'})}{ P(\\tau;\\theta)}f(\\tau) $$\n",
    "This allows to interpret the equation as the coefficient for sampling under the old policy, with an extra re-weighting factor in addition to just averaging.  \n",
    "!! We need to make sure that the re-weighting factor is not too far from 1.\n",
    "\n",
    "### 4. The surrogate function:\n",
    "With all previous ideas taken into consideration, if we want to update our policy 𝜋𝜃′ and we only have trajectories generated by an older policy we use importance sampling and compute the gradient as follows:\n",
    "$$ g = \\frac{ P(\\tau;\\theta^{'})}{ P(\\tau;\\theta)} \\sum_{t} \\frac{\\nabla_{\\theta^{'}} \\pi_{\\theta^{'}}(a_{t}|s_{t})}{ \\pi_{\\theta^{'}}(a_{t}|s_{t})} R_{t}^{future}$$  \n",
    "The re-weighting factor is just the product of all the policy accross each step:\n",
    "$$ g = \\sum_{t}  \\frac{... \\pi_{\\theta^{'}}(a_{t}|s_{t}) ...} {... \\pi_{\\theta}(a_{t}|s_{t}) ...}  \\frac{\\nabla_{\\theta^{'}} \\pi_{\\theta^{'}}(a_{t}|s_{t})}{ \\pi_{\\theta^{'}}(a_{t}|s_{t})} R_{t}^{future}$$  \n",
    "And if we rearrange the equation a little bit more: \n",
    "$$ g = \\sum_{t}  \\frac{... \\pi_{\\theta^{'}}(a_{t}|s_{t}) ...} {... \\pi_{\\theta^{'}}(a_{t}|s_{t})  ...}  \\frac{\\nabla_{\\theta^{'}} \\pi_{\\theta}(a_{t}|s_{t})}{ \\pi_{\\theta}(a_{t}|s_{t})} R_{t}^{future}$$ \n",
    "We can cancel the terms on the left, but still we are left with a product of policies at different times. And here where PPO comes in, if the old and the current policy are close to each other, all these . . . factors will be pretty close to one, then we can ignore them and the equation simplifies even further:\n",
    "$$ g = \\sum_{t} \\frac{\\nabla_{\\theta^{'}} \\pi_{\\theta}(a_{t}|s_{t})}{ \\pi_{\\theta}(a_{t}|s_{t})} R_{t}^{future}$$ \n",
    "Now that we have an approximate form of the gradient, we can think of it as a gradient of a new object which is: \n",
    "$$ g =\\nabla_{\\theta^{'}} \\sum_{t} \\frac{ \\pi_{\\theta^{'}}(a_{t}|s_{t})}{ \\pi_{\\theta}(a_{t}|s_{t})} R_{t}^{future}$$\n",
    "$$ g =\\nabla_{\\theta^{'}} L_{sur}(\\theta^{'},\\theta)$$  \n",
    "**To sum up here:** The vanilla policy gradient method uses the log probability of the actions log 𝜋𝜃(𝑎𝑡|𝑠𝑡) to trace the impact of the actions, but here another function is doing this. 𝐿𝑠𝑢𝑟(𝜃′,𝜃) uses the probability of the action under the current policy 𝜋𝜃′(𝑎|𝑠), divided by the probability of the action under your previous policy 𝜋𝜃(𝑎|𝑠).  \n",
    "\n",
    "### 5. The clipped surrogate function:\n",
    "if we keep re-using old trajectories and updating our policy, at some point the new policy might become different enough from the old one, that may result in having all the approximations we made could become invalid. Also, the raio  will tend to be really big and lead to taking big gradient steps that might wreck the policy after the update.\n",
    "Instead of adding KL-divergence such as in TRPO, what about building these properties into the objective function - itself:\n",
    "$$L_{sur}^{clip}(\\theta,\\theta^{'}) = \\sum_{t} min(  \\frac{ \\pi_{\\theta^{'}}(a_{t}|s_{t})}{ \\pi_{\\theta}(a_{t}|s_{t})} R_{t}^{future}, clip_{\\epsilon}(\\frac{ \\pi_{\\theta^{'}}(a_{t}|s_{t})}{ \\pi_{\\theta}(a_{t}|s_{t})}) R_{t}^{future}   )$$  \n",
    "- The first term inside the minimization is the same ratio we saw in the previous 𝐿𝑠𝑢𝑟(𝜃′,𝜃) objective (it's the same as TRPO objective).\n",
    "- The second term is a version where the the ratio is clipped between (1 - 𝜖, 1 + 𝜖). (in the paper they state a good value for 𝜖 is about 0.2, so r can vary between ~(0.8, 1.2)). \n",
    "- Then, finally, the minimization of both of these terms is taken.\n",
    "\n",
    "### 6. PPO algorithm:\n",
    "![PPO](img/PPO.png)\n",
    "\n",
    "1. Collect some trajectories based on some policy 𝜋𝜃(𝑎|𝑠), and initialize θ′=θ\n",
    "2. Next, compute the gradient of the clipped surrogate function using the trajectories\n",
    "3. Update θ′ using gradient ascent θ′← θ′+α∇θ′Lsurclip(θ′,θ)\n",
    "4. Then we repeat step 2-3 without generating new trajectories. Typically, step 2-3 are only repeated a few times\n",
    "5. Set θ=θ′, go back to step 1, repeat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
