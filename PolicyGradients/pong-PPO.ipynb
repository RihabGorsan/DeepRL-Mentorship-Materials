{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting JSAnimation\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n",
      "Building wheels for collected packages: JSAnimation\n",
      "  Building wheel for JSAnimation (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/rihab/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n",
      "Successfully built JSAnimation\n",
      "Installing collected packages: JSAnimation\n",
      "Successfully installed JSAnimation-0.1\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "\n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training: a function for collecting trajecories and also a function for calculating the scalar clipped surrogate function. \n",
    "\n",
    "## 1. Collect trajectories function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, \\\n",
    "        action_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  The cliiped surrogate function: \n",
    "Implemente the clipped surrogate function given by: \n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```   \n",
    "\n",
    "### Entropy term:\n",
    "But before starting with the clipped surrogate function, let's recall what entropy is and how it could be very useful for exploration in PPO.\n",
    "Even with the policy represented as probability distribution, there is a high chance that the agent will converge to some locally optimal policy and stop exploring the env. For example in value based methods such as DQN we solved this using epsilon greedy action selection. One could think that we could do the same but PG allows us to follow a better path using entropy bonus.\n",
    "\n",
    "Recall that **entropy** is a measure of uncertainty in some system. Being applied to the agent policy, the entropy shows how much the agent is uncertain about which action to take.\n",
    "In math notation, entropy of the policy is defined as $H(\\pi) = - \\sum_{\\tau} \\pi(a|s) log(\\pi(a|s))$\n",
    "\n",
    "⇒ To prevent our agent from being stuck in the local minimum (i.e: isn't exploring enough the search space), we can substract the entropy from the loss function, punishing the agent from being too certain about the action to take.  \n",
    "\n",
    "⇒ This exploration strategy introduces new hyperparameter called ```entropy_beta```. It is the scale of the entropy bonus.\n",
    "\n",
    "⇒ After computing the entropy loss as follows ```entropyLoss = entropy_beta * (-(prob_v * log_prob_v).sum(dim=1).mean())```, we can substract it from the policy loss like this: ```policyLoss -= entropyLoss```  \n",
    "\n",
    "One can track the the entropy scalar and plot it to assess training. The entropy should be decreasing during the training, this shows that the policy is moving from the uniform distribution to more deterministic actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    #\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1      # the starting value for the clipping epsilon, epsilon is decayed as time goes on\n",
    "beta = .01         # the scale of the entropy bonus. \n",
    "tmax = 320         # timelimit for episode\n",
    "SGD_epoch = 4      # controlls how many times we use the same trajectory to update the policy\n",
    "                   # can vary between 4 and 8 \n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "    # compute total rewards \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
